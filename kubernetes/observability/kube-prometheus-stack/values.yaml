# Values passed to the kube-prometheus-stack subchart
kube-prometheus-stack:
  # Disable components we don't need or that conflict with Talos
  kubeControllerManager:
    enabled: true
    endpoints:
      - 192.168.30.50
    service:
      enabled: true
      port: 10257
      targetPort: 10257
    serviceMonitor:
      enabled: true
      https: true
      insecureSkipVerify: true
  
  kubeScheduler:
    enabled: true
    endpoints:
      - 192.168.30.50
    service:
      enabled: true
      port: 10259
      targetPort: 10259
    serviceMonitor:
      enabled: true
      https: true
      insecureSkipVerify: true
  
  kubeEtcd:
    enabled: true
    endpoints:
      - 192.168.30.50
    service:
      enabled: true
      port: 2381
      targetPort: 2381
  
  kubeProxy:
    enabled: false  # Replaced by Cilium
  
  # Grafana configuration
  grafana:
    enabled: true
    adminPassword: ""  # Set via secret
    admin:
      existingSecret: grafana-admin
      userKey: admin-user
      passwordKey: admin-password
    
    # Inject OAuth client secret as environment variable
    envValueFrom:
      GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET:
        secretKeyRef:
          name: grafana-admin
          key: oauth-client-secret
    
    # Grafana configuration
    grafana.ini:
      server:
        root_url: https://grafana.int.lorenzodebie.be
      auth:
        disable_login_form: false  # Keep local admin as fallback
      auth.generic_oauth:
        enabled: true
        name: Authentik
        allow_sign_up: true
        client_id: grafana
        # client_secret injected via GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET env var
        scopes: openid email profile groups
        auth_url: https://auth.int.lorenzodebie.be/application/o/authorize/
        token_url: https://auth.int.lorenzodebie.be/application/o/token/
        api_url: https://auth.int.lorenzodebie.be/application/o/userinfo/
        # JMESPath: Admin if in grafana-admins group, otherwise deny login
        role_attribute_path: contains(groups[*], 'grafana-admins') && 'Admin' || 'Deny'
        allow_assign_grafana_admin: true
        # TODO: Re-enable TLS verification after Cloudflare migration
        tls_skip_verify_insecure: true
    
    # Disable init-chown-data container - chown fails on NFS with Mapall
    initChownData:
      enabled: false
    
    # Run as UID/GID 3001 (media user) to match NFS Mapall setting
    securityContext:
      runAsUser: 3001
      runAsGroup: 3001
      fsGroup: 3001
    
    persistence:
      enabled: true
      storageClassName: nfs-config
      size: 5Gi
    
    sidecar:
      dashboards:
        enabled: true
        searchNamespace: ALL
      datasources:
        enabled: true
        searchNamespace: ALL
    
    # Additional data sources
    additionalDataSources:
      - name: Loki
        type: loki
        url: http://loki.observability.svc.cluster.local:3100
        access: proxy
        isDefault: false
    
    # Dashboard providers for custom folders
    dashboardProviders:
      dashboardproviders.yaml:
        apiVersion: 1
        providers:
          - name: 'grafana-dashboards-custom'
            orgId: 1
            folder: 'Custom'
            type: file
            disableDeletion: false
            editable: true
            options:
              path: /var/lib/grafana/dashboards/grafana-dashboards-custom
    
    # Custom dashboards from grafana.com
    dashboards:
      grafana-dashboards-custom:
        # Node Exporter Full - Detailed node hardware metrics
        node-exporter-full:
          gnetId: 1860
          revision: 37
          datasource: Prometheus
        
        # Loki Logs - Log exploration dashboard
        loki-logs:
          gnetId: 13639
          revision: 2
          datasource: Loki
        
        # Loki & Promtail - Promtail health and log rates
        loki-promtail:
          gnetId: 10880
          revision: 1
          datasource: Prometheus
        
        # Cilium Agent Metrics - CNI agent monitoring
        # Note: Requires Cilium prometheus.serviceMonitor.enabled=true
        cilium-agent:
          gnetId: 16611
          revision: 1
          datasource: Prometheus
        
        # Cilium Operator Metrics - Cilium operator monitoring
        # Note: Requires Cilium prometheus.serviceMonitor.enabled=true
        cilium-operator:
          gnetId: 16612
          revision: 1
          datasource: Prometheus
        
        # ArgoCD - GitOps application sync status
        # Note: Requires ArgoCD serviceMonitor.enabled=true
        argocd:
          gnetId: 14584
          revision: 1
          datasource: Prometheus
        
        # Authentik - Authentication metrics
        # Note: Requires Authentik server.metrics.serviceMonitor.enabled=true
        authentik:
          gnetId: 14837
          revision: 2
          datasource: Prometheus
  
  # Prometheus configuration
  prometheus:
    prometheusSpec:
      retention: 30d
      retentionSize: 50GB
      
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: nfs-config
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: 50Gi
      
      # Scrape all ServiceMonitors
      serviceMonitorSelectorNilUsesHelmValues: false
      podMonitorSelectorNilUsesHelmValues: false
      ruleSelectorNilUsesHelmValues: false
      
      # Resource limits
      resources:
        requests:
          cpu: 200m
          memory: 1Gi
        limits:
          memory: 4Gi
  
  # Alertmanager configuration
  alertmanager:
    enabled: true

    # Alertmanager routing and receiver configuration
    config:
      global:
        resolve_timeout: 5m

      # Night hours - alerts are muted during this period
      # Split into two intervals because start_time must be before end_time
      time_intervals:
        - name: nighttime
          time_intervals:
            # Late night: 23:00 - midnight
            - times:
                - start_time: '23:00'
                  end_time: '24:00'
              weekdays: ['sunday:saturday']
              location: 'Europe/Brussels'
            # Early morning: midnight - 08:00
            - times:
                - start_time: '00:00'
                  end_time: '08:00'
              weekdays: ['sunday:saturday']
              location: 'Europe/Brussels'

      # Routing configuration
      route:
        receiver: 'telegram'
        group_by: ['alertname', 'namespace', 'severity']
        group_wait: 30s
        group_interval: 5m
        repeat_interval: 4h
        routes:
          # Critical alerts: faster repeat, but still muted at night
          - receiver: 'telegram'
            matchers:
              - severity="critical"
            repeat_interval: 1h
            mute_time_intervals:
              - nighttime
            continue: false

          # Warning alerts: standard timing
          - receiver: 'telegram'
            matchers:
              - severity="warning"
            repeat_interval: 4h
            mute_time_intervals:
              - nighttime
            continue: false

          # Info/other alerts: less frequent
          - receiver: 'telegram'
            repeat_interval: 12h
            mute_time_intervals:
              - nighttime

      # Notification receivers
      receivers:
        - name: 'telegram'
          telegram_configs:
            - bot_token_file: /etc/alertmanager/secrets/alertmanager-telegram/bot-token
              chat_id: "8472055654"  
              send_resolved: true
              parse_mode: HTML
              message: |-
                {{ if eq .Status "firing" }}ðŸ”´{{ else }}ðŸŸ¢{{ end }} <b>{{ .Status | toUpper }}</b>

                {{ range .Alerts }}
                <b>{{ .Labels.alertname }}</b>
                {{ if .Annotations.summary }}{{ .Annotations.summary }}{{ end }}
                {{ if .Annotations.description }}
                <i>Details:</i> {{ .Annotations.description }}{{ end }}
                <i>Severity:</i> {{ .Labels.severity | default "unknown" }}
                {{ if .Labels.namespace }}<i>Namespace:</i> {{ .Labels.namespace }}{{ end }}
                {{ if .Labels.pod }}<i>Pod:</i> {{ .Labels.pod }}{{ end }}
                {{ end }}

      # Inhibition rules - suppress lower severity alerts when higher ones fire
      inhibit_rules:
        - source_matchers:
            - severity="critical"
          target_matchers:
            - severity="warning"
          equal: ['alertname', 'namespace']

    alertmanagerSpec:
      # Mount the Telegram secret so Alertmanager can read the bot token
      secrets:
        - alertmanager-telegram

      storage:
        volumeClaimTemplate:
          spec:
            storageClassName: nfs-config
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: 1Gi
